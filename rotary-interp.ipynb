{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2zafMz0DjDszPIvFROgJY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"id":"rOB6mtHK8pek","executionInfo":{"status":"ok","timestamp":1683042046998,"user_tz":420,"elapsed":2,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import functional as F\n","from fancy_einsum import einsum\n","import torch.utils.data as data"]},{"cell_type":"code","source":["!pip install rotary-embedding-torch\n","!pip install fancy_einsum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e67WeQ180qH","executionInfo":{"status":"ok","timestamp":1683041270089,"user_tz":420,"elapsed":10645,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}},"outputId":"71a005aa-60c3-405d-8757-7c592f399bdc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (0.2.2)\n","Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.6.1)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->rotary-embedding-torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->rotary-embedding-torch) (16.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->rotary-embedding-torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->rotary-embedding-torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fancy_einsum\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Installing collected packages: fancy_einsum\n","Successfully installed fancy_einsum-0.0.3\n"]}]},{"cell_type":"code","source":["# data"],"metadata":{"id":"BF0Fn9TFFTv0","executionInfo":{"status":"ok","timestamp":1683041372194,"user_tz":420,"elapsed":2,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class PreviousTokenDataset(data.Dataset):\n","    def __init__(self, n_examples, n_tokens):\n","      self.samples = []\n","      for _ in range(n_examples):\n","        r = torch.randint(high=10, size=(n_tokens,))\n","        label = r[-2]\n","        self.samples.append((r, label))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, index):\n","        return self.samples[index]\n","\n","def create_dataloaders(batch_size, n_examples, train_val_split, n_tokens, num_workers=1):\n","    train_dataset = PreviousTokenDataset(n_examples, n_tokens)\n","    val_dataset = PreviousTokenDataset(int(n_examples * train_val_split), n_tokens)\n","\n","    train_loader = data.DataLoader(train_dataset, batch_size=batch_size,\n","                                   shuffle=True, num_workers=num_workers)\n","    val_loader = data.DataLoader(val_dataset, batch_size=batch_size,\n","                                 shuffle=False, num_workers=num_workers)\n","\n","    return train_loader, val_loader\n","\n","batch_size = 64\n","num_workers = 1\n","n_examples = 1000\n","train_val_split = 0.2\n","n_tokens = 3\n","\n","train_loader, val_loader = create_dataloaders(batch_size, n_examples, train_val_split, n_tokens)\n"],"metadata":{"id":"fNJNViDPFVLy","executionInfo":{"status":"ok","timestamp":1683042991770,"user_tz":420,"elapsed":178,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def test_previous_token_dataset():\n","  batch_size = 64\n","  num_workers = 1\n","  n_examples = 1000\n","  train_val_split = 0.2\n","  n_tokens = 3\n","\n","  train_loader, val_loader = create_dataloaders(batch_size, n_examples, train_val_split, n_tokens)\n","\n","  assert len(train_loader.dataset) == n_examples, \"Training dataset size is incorrect.\"\n","\n","  for inputs, labels in train_loader:\n","      assert inputs.size(1) == n_tokens, f\"Input tensor has wrong number of tokens: {inputs.size(1)}\"\n","      assert inputs.size(0) == batch_size or inputs.size(0) == len(train_loader.dataset) % batch_size, \"Batch size is incorrect.\"\n","      print(labels.shape)\n","      print(inputs.shape)\n","      assert labels[0] == inputs[0][-2]\n","      break\n","\n","  for inputs, labels in val_loader:\n","      assert inputs.size(1) == n_tokens, f\"Input tensor has wrong number of tokens: {inputs.size(1)}\"\n","      assert inputs.size(0) == batch_size or inputs.size(0) == len(val_loader.dataset) % batch_size, \"Batch size is incorrect.\"\n","      break\n","\n","test_previous_token_dataset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZG9vNZakH-eF","executionInfo":{"status":"ok","timestamp":1683042956699,"user_tz":420,"elapsed":175,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}},"outputId":"c8a13c3b-ecd3-4514-efec-4578ad74a918"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64])\n","torch.Size([64, 3])\n"]}]},{"cell_type":"code","source":["\n","@dataclass\n","class Config:\n","    d_model: int = 768\n","    debug: bool = True\n","    layer_norm_eps: float = 1e-5\n","    d_vocab: int = 50257\n","    init_range: float = 0.02\n","    n_ctx: int = 1024\n","    d_head: int = 64\n","    d_mlp: int = 3072\n","    n_heads: int = 12\n","    n_layers: int = 12\n","\n","cfg = Config()\n","print(cfg)"],"metadata":{"id":"c_Ewfp6zFofi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_embd = 64\n","num_heads = 4\n","head_size = n_embd / num_heads\n","dropout = 0.8\n","vocab_size = 128\n","block_size = 3"],"metadata":{"id":"BWB6PQPb_mgA","executionInfo":{"status":"ok","timestamp":1683039993911,"user_tz":420,"elapsed":3,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# GPT Neo-X Rotary Implementation\n","class Rotary(torch.nn.Module):\n","    def __init__(self, dim, base=10000):\n","        super().__init__()\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)\n","        self.seq_len_cached = None\n","        self.cos_cached = None\n","        self.sin_cached = None\n","\n","    def forward(self, x, seq_dim=1):\n","        seq_len = x.shape[seq_dim]\n","        if seq_len != self.seq_len_cached:\n","            self.seq_len_cached = seq_len\n","            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n","            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n","            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n","            self.cos_cached = emb.cos()[:, None, None, :]\n","            self.sin_cached = emb.sin()[:, None, None, :]\n","        return self.cos_cached, self.sin_cached\n","\n","\n","# rotary pos emb helpers:\n","\n","def rotate_half(x):\n","    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n","    return torch.cat(\n","        (-x2, x1), dim=x1.ndim - 1\n","    )  # dim=-1 triggers a bug in torch < 1.8.0\n","\n","\n","@torch.jit.script\n","def apply_rotary_pos_emb(q, k, cos, sin):\n","    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n","\n"],"metadata":{"id":"w2rQMW1W8sva","executionInfo":{"status":"ok","timestamp":1683039187628,"user_tz":420,"elapsed":172,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Train a 1L attention-only transformer with rotary embeddings to predict the previous token"],"metadata":{"id":"-mFD5pmC8--k","executionInfo":{"status":"ok","timestamp":1683039242274,"user_tz":420,"elapsed":4,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # input of size (batch, time-step, channels)\n","        # output of size (batch, time-step, head size)\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,hs)\n","        q = self.query(x) # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class Unembed(nn.Module):\n","    def __init__(self, init_range=1):\n","        super().__init__()\n","        self.W_U = nn.Parameter(torch.empty((n_embd, vocab_size)))\n","        nn.init.normal_(self.W_U, std=init_range)\n","        self.b_U = nn.Parameter(torch.zeros((vocab_size), requires_grad=False))\n","    \n","    def forward(self, normalized_resid_final):\n","        # normalized_resid_final [batch, position, d_model]\n","        if self.cfg.debug: print(\"Normalized_resid_final:\", normalized_resid_final.shape)\n","        logits = einsum(\"batch position d_model, d_model d_vocab -> batch position d_vocab\", normalized_resid_final, self.W_U) + self.b_U\n","        return logits"],"metadata":{"id":"3mJNjkhf9MRa","executionInfo":{"status":"ok","timestamp":1683041363659,"user_tz":420,"elapsed":157,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from torch.nn.modules.activation import MultiheadAttention\n","# https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n","class RotaryAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.token_embedding = nn.Embedding(vocab_size, n_embd)\n","    self.rotary_emb = Rotary(n_embd)\n","    self.attention = MultiheadAttention(num_heads, head_size, dropout)\n","    self.unembed = Unembed()\n","\n","  def forward(self, idx):\n","    B, T = idx.shape\n","    tok_emb = self.token_embedding(idx) # (B,T,C)\n","    pos_emb = self.rotary_emb(idx)\n","    x = tok_emb + pos_emb\n","    x += self.attention(x)\n","    return self.unembed(x)\n"],"metadata":{"id":"LwBowQnj-u49","executionInfo":{"status":"ok","timestamp":1683043079534,"user_tz":420,"elapsed":1,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["model = RotaryAttention()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"y8MPrKl1LnrX","executionInfo":{"status":"error","timestamp":1683043079862,"user_tz":420,"elapsed":171,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}},"outputId":"74f261ba-d8e3-4d04-fef3-c4142d45b133"},"execution_count":54,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-9037e7307832>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRotaryAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-8500b0497a7f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRotary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munembed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"]}]},{"cell_type":"code","source":["\n","for inputs, batch in train_loader:\n","  pass\n"],"metadata":{"id":"IPQqVVGiFSkc","executionInfo":{"status":"ok","timestamp":1683043000613,"user_tz":420,"elapsed":3,"user":{"displayName":"Eric Walden","userId":"08238851542227917780"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_osj2ApLeVv"},"execution_count":null,"outputs":[]}]}